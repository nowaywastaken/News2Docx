# News2Docx example config (copy to config.yml and edit)

# Crawler
# crawler_mode: remote | local (default: remote)
crawler_mode: remote
# When mode=remote
crawler_api_url: "PUT_YOUR_CRAWLER_API_URL_HERE"
crawler_api_token: "PUT_YOUR_CRAWLER_API_TOKEN_HERE"
# When mode=local (directly query GDELT; may be blocked in mainland China)
crawler_sites_file: server/news_website.txt
# Optional GDELT params (local mode)
# gdelt_timespan: "7d"        # e.g., 24h / 7d / 30d
# gdelt_max_per_call: 50       # per batch
# gdelt_sort: datedesc         # datedesc | dateasc

# LLM (OpenAI-Compatible)
openai_api_base: "https://api.siliconflow.cn/v1"
openai_api_key: "PUT_YOUR_OPENAI_COMPATIBLE_API_KEY_HERE"

# Scrape
max_urls: 3
concurrency: 4
retry_hours: 24
timeout: 30
strict_success: true
max_api_rounds: 5
per_url_retries: 2
pick_mode: random          # random | top
random_seed: 42            # optional: fixed random seed
db_path: .n2d_cache/crawled.sqlite3  # store crawled URLs to avoid duplicates
noise_patterns:            # optional: extra site-noise patterns appended to built-in list
  - "please refresh"
  - "cookie"

# Process
target_language: Chinese
merge_short_paragraph_chars: 80   # merge English paragraphs shorter than this with a short neighbor

# Export
run_export: true                 # same as CLI: --export
export_split: true               # same as CLI: --split (one DOCX per article)
export_order: en-zh              # same as CLI: --order en-zh (English first)
export_mono: false               # set true for Chinese-only
export_out_dir:                  # optional; default is user Desktop/鑻辨枃鏂伴椈绋?
export_first_line_indent_cm: 0.74
export_font_zh_name: 瀹嬩綋
export_font_zh_size: 10.5
export_font_en_name: Cambria
export_font_en_size: 10.5
export_title_bold: true
export_title_size_multiplier: 1.0

