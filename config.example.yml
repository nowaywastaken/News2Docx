# News2Docx configuration example (copy to `config.yml` for use)
# All values are plain text here; when using `config.yml`, sensitive fields are auto-encrypted at rest
# by `news2docx.infra.secure_config.secure_load_config` via system keyring.

app:
  name: "News2Docx UI"

ui:
  fixed_width: 350
  fixed_height: 600
  high_dpi: true
  initial_page: 0
  theme:
    primary_color: "#4C7DFF"
    background_color: "#F7F9FC"
    text_color: "#1F2937"

# Security: field-level encryption (applies only to config.yml)
security:
  enable_encryption: true
  keyring_service: "News2Docx"
  sensitive_keys:
    - openai_api_key
    - crawler_api_token
    - crawler_api_url

# Scraper: choose ONE mode. Remote requires a token; Local uses GDELT with a sites list file.
# Remote mode (recommended if you have a working endpoint + token)
crawler_mode: remote   # remote | local
crawler_api_url: "https://gdelt-xupojkickl.cn-hongkong.fcapp.run"
crawler_api_token: "PUT_YOUR_CRAWLER_API_TOKEN_HERE"

# Local mode (uncomment to use). Requires a domain list file; one domain per line.
# crawler_mode: local
# crawler_sites_file: server/news_website.txt
# gdelt_timespan: "7d"             # e.g., 7d, 24h
# gdelt_max_per_call: 50
# gdelt_sort: datedesc              # datedesc | datedescdeprecated

# Scrape controls
max_urls: 10                        # number of URLs to process per run
concurrency: 4                      # thread pool size
timeout: 30                         # per-request timeout (seconds)
pick_mode: random                   # random | top
random_seed: null                   # integer or null
db_path: ".n2d_cache/crawled.sqlite3"  # deduplicate previously processed URLs
noise_patterns: []                  # extra noise filters; enter plain text or regex
                                    # - Plain: use Chinese or English comma to separate, e.g. 关键词1，关键词2
                                    # - Regex: prefix with `re:` or wrap with `/.../`, e.g. re:^subscribe$ 或 /cookie(s)?/i

# Processing (OpenAI-compatible API). Model and API key are required.
openai_api_base: "https://api.siliconflow.cn/v1"
openai_api_key: "PUT_YOUR_OPENAI_COMPATIBLE_API_KEY_HERE"
# Optional split models (recommended): set both for precise control
openai_model_general: "PUT_YOUR_GENERAL_MODEL_ID"      # e.g., gpt-4o-mini（英文编辑/压词）
openai_model_translation: "PUT_YOUR_TRANSLATION_MODEL_ID"  # e.g., qwen-2.5-7b-instruct（翻译）
# Legacy single model (backward compatible). If set, it is used as fallback for both above.
openai_model: ""      # e.g., gpt-4o-mini or vendor-specific
target_language: "Chinese"                  # target translation language
merge_short_paragraph_chars: 80             # merge very short English paragraphs before translation

# Processing cleanup rules (line-wise filtering applied before export)
processing_forbidden_prefixes: ["Note:", "Source:", "Copyright", "Image:"]
processing_forbidden_patterns: ["^\u3010.*?\u3011.*$"]  # example: bracketed tags
processing_min_words_after_clean: 200

# Export settings
export_split: true                   # true: per-article files; false: single DOCX
export_order: zh-en                  # zh-en | en-zh (bilingual order if not mono)
export_mono: false                   # true: only Chinese output
export_out_dir: null                 # null => defaults to Desktop/英文新闻稿
export_first_line_indent_inch: 0.2
export_font_zh_name: "SimSun"
export_font_zh_size: 10.5
export_font_en_name: "Cambria"
export_font_en_size: 10.5
export_title_bold: true
export_title_size_multiplier: 1.0

# Runs directory (where scraped/processed artifacts are stored)
runs_dir: "runs"
