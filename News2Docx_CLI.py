#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»çˆ¬è™«ä¸æ–‡æ¡£ç”Ÿæˆä¸»ç¨‹åº
ç›´æ¥æ‰§è¡Œå®Œæ•´çš„æ–°é—»å¤„ç†æµç¨‹ï¼šçˆ¬å– -> AIå¤„ç† -> æ–‡æ¡£ç”Ÿæˆ
åˆå¹¶äº†main.pyã€news_pipeline.pyå’Œdocx_writer.pyçš„æ‰€æœ‰åŠŸèƒ½

é›†æˆScraper.pyæ¨¡å—ï¼šScraper.pyæä¾›çº¯çˆ¬è™«åŠŸèƒ½ï¼ŒNews2Docx_CLI.pyæä¾›AIå¤„ç†åŠŸèƒ½
"""

# æ ‡å‡†åº“å¯¼å…¥
import sys
import os
import re
import json
import time
import uuid
import random
import logging
import argparse
import glob
import itertools
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from urllib.parse import urlparse
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn

# ä»Scraper.pyå¯¼å…¥çˆ¬è™«ç›¸å…³åŠŸèƒ½
from Scraper import (
    ScrapeConfig, ScrapeResults,
    HttpClient, ContentExtractor, NewsAPIService, URLStore,
    NewsProcessingError, ScrapingError, APIError,
    now_stamp, ensure_directory,
    build_arg_parser, save_scraped_data_to_json
)
from Scraper import NewsScraper as ScraperNewsScraper

# ä»ai_processor.pyå¯¼å…¥AIå¤„ç†ç›¸å…³åŠŸèƒ½
from ai_processor import (
    Article, ProcessingError, DocumentError,
    estimate_max_tokens, build_source_block, build_system_and_user_prompt,
    backoff_sleep, call_siliconflow_tagged_output,  # ä¿®æ­£å‡½æ•°å
    generate_summary_single_article, repair_summary_if_too_short,
    process_single_article, split_articles_blocks, parse_paragraphs,
    validate_and_adjust_word_count, build_final_json_struct,
    process_articles_concurrent, process_articles_with_two_steps,
    process_articles_to_words, count_english_words, safe_filename,
    handle_error, safe_execute, now_stamp,
    DEFAULT_MODEL_ID, DEFAULT_SILICONFLOW_API_KEY, DEFAULT_SILICONFLOW_URL,  # ä¿®æ­£å¸¸é‡å
    DEFAULT_BATCH_SIZE, DEFAULT_MAX_TOKENS_HARD_CAP, DEFAULT_USE_TWO_STEP,
    DEFAULT_CONCURRENCY, CHINESE_VARIANT_HINT, SEPARATOR_LINE, MAIN_SEPARATOR,
    TAG_RE, PARA_TAG_RE,
    # æ–°å¢çš„ä¸¤æ­¥å¤„ç†å‡½æ•°
    process_articles_two_steps_concurrent, TARGET_WORD_MIN, TARGET_WORD_MAX
)

# ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿå¯¼å…¥
from unified_logger import (
    get_unified_logger, log_task_start, log_task_end,
    log_processing_step, log_error, log_performance,
    unified_print, log_processing_result, log_file_operation,
    log_batch_processing
)


# -------------------------------
# æ•°æ®æ ¼å¼è½¬æ¢å‡½æ•°
# -------------------------------

def convert_ai_processor_output_to_docx_format(processed_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†ai_processorçš„è¾“å‡ºè½¬æ¢ä¸ºNews2Docx_CLIæœŸæœ›çš„æ ¼å¼

    Args:
        processed_data: ai_processorçš„è¾“å‡ºæ•°æ®

    Returns:
        Dict[str, Any]: è½¬æ¢ä¸ºNews2Docx_CLIæœŸæœ›æ ¼å¼çš„æ•°æ®
    """
    if not processed_data or "articles" not in processed_data:
        return processed_data

    converted_articles = []

    for article in processed_data["articles"]:
        try:
            # è½¬æ¢å•ç¯‡æ–‡ç« 
            converted_article = convert_single_article_for_docx(article)
            if converted_article:
                converted_articles.append(converted_article)
        except Exception as e:
            unified_print(f"è½¬æ¢æ–‡ç«  {article.get('id', 'unknown')} æ—¶å‡ºé”™: {e}", "news2docx_cli", "format_conversion", "warning")
            continue

    # è¿”å›è½¬æ¢åçš„æ•°æ®ç»“æ„
    return {
        "articles": converted_articles,
        "metadata": processed_data.get("metadata", {}),
        "total": len(converted_articles),
        "success": len(converted_articles),
        "failed": len(processed_data.get("articles", [])) - len(converted_articles)
    }


def convert_single_article_for_docx(article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    å°†å•ç¯‡æ–‡ç« ä»ai_processoræ ¼å¼è½¬æ¢ä¸ºNews2Docx_CLIæ ¼å¼

    Args:
        article: å•ç¯‡æ–‡ç« æ•°æ®ï¼ˆai_processoræ ¼å¼ï¼‰

    Returns:
        Optional[Dict[str, Any]]: è½¬æ¢åçš„æ–‡ç« æ•°æ®ï¼Œè½¬æ¢å¤±è´¥è¿”å›None
    """
    try:
        article_id = article.get("id", "unknown")

        # è·å–è‹±æ–‡æ ‡é¢˜
        eng_title = article.get("original_title", "").strip()

        # è·å–è‹±æ–‡å†…å®¹
        eng_content_raw = article.get("adjusted_content", article.get("original_content", ""))

        # è·å–ç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜å’Œå†…å®¹
        translated_title = article.get("translated_title", "").strip()
        translated_content = article.get("translated_content", "")

        # å¦‚æœæ²¡æœ‰ç¿»è¯‘å†…å®¹ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹
        if not translated_content:
            chi_title = translated_title if translated_title else eng_title  # ä½¿ç”¨ç¿»è¯‘æ ‡é¢˜æˆ–è‹±æ–‡æ ‡é¢˜ä½œä¸ºä¸­æ–‡æ ‡é¢˜
            chi_content_raw = eng_content_raw
        else:
            # ä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»ç¿»è¯‘å†…å®¹ä¸­æå–
            if translated_title:
                chi_title = translated_title
                chi_content_raw = translated_content
            else:
                # å°è¯•ä»ç¿»è¯‘å†…å®¹ä¸­æå–ä¸­æ–‡æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œï¼‰
                lines = translated_content.split('\n', 1)
                chi_title = lines[0].strip() if lines else eng_title
                # ä¸­æ–‡å†…å®¹æ˜¯é™¤æ ‡é¢˜å¤–çš„å…¶ä½™éƒ¨åˆ†
                chi_content_raw = lines[1] if len(lines) > 1 else translated_content

        # æŒ‰æ®µè½åˆ†å‰²è‹±æ–‡å†…å®¹
        eng_content_dict = split_content_into_paragraphs(eng_content_raw, "english")

        # æŒ‰æ®µè½åˆ†å‰²ä¸­æ–‡å†…å®¹
        chi_content_dict = split_content_into_paragraphs(chi_content_raw, "chinese")

        # æ„å»ºè½¬æ¢åçš„æ–‡ç« æ•°æ®
        converted_article = {
            "id": article_id,
            "EngTitle": eng_title,
            "ChiTitle": chi_title,
            "EngContent": eng_content_dict,
            "ChiContent": chi_content_dict,
            "url": article.get("url", ""),
            "processing_timestamp": article.get("processing_timestamp", ""),
            "target_language": article.get("target_language", "Chinese"),
            "success": article.get("success", True)
        }

        return converted_article

    except Exception as e:
        unified_print(f"è½¬æ¢æ–‡ç« æ—¶å‡ºé”™: {e}", "news2docx_cli", "format_conversion", "error")
        return None


def split_content_into_paragraphs(content: str, language: str = "english") -> Dict[str, str]:
    """
    å°†æ–‡æœ¬å†…å®¹æŒ‰æ®µè½åˆ†å‰²æˆå­—å…¸æ ¼å¼

    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        language: è¯­è¨€ç±»å‹ ("english" æˆ– "chinese")

    Returns:
        Dict[str, str]: æ®µè½å­—å…¸ {"p1": "content1", "p2": "content2", ...}
    """
    if not content:
        return {}

    # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²æ®µè½
    paragraphs = [p.strip() for p in content.split('\n') if p.strip()]

    # å¦‚æœæ²¡æœ‰æ¢è¡Œç¬¦ï¼ŒæŒ‰å¥å­åˆ†å‰²ï¼ˆå¯¹äºè‹±æ–‡ï¼‰
    if len(paragraphs) == 1 and language == "english":
        # è‹±æ–‡æŒ‰å¥å·åˆ†å‰²
        sentences = re.split(r'(?<=[.!?])\s+', content)
        paragraphs = [s.strip() for s in sentences if s.strip()]

    # å¦‚æœè¿˜æ˜¯åªæœ‰ä¸€ä¸ªæ®µè½ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰%%åˆ†éš”ç¬¦
    if len(paragraphs) == 1:
        if '%%' in content:
            paragraphs = [p.strip() for p in content.split('%%') if p.strip()]

    # æ„å»ºæ®µè½å­—å…¸
    paragraph_dict = {}
    for i, para in enumerate(paragraphs, 1):
        if para:  # åªæ·»åŠ éç©ºæ®µè½
            paragraph_dict[f"p{i}"] = para

    return paragraph_dict


def convert_scraper_output_to_docx_format(scraper_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†Scraperçš„è¾“å‡ºè½¬æ¢ä¸ºNews2Docx_CLIæœŸæœ›çš„æ ¼å¼

    Args:
        scraper_data: Scraperçš„è¾“å‡ºæ•°æ®

    Returns:
        Dict[str, Any]: è½¬æ¢ä¸ºNews2Docx_CLIæœŸæœ›æ ¼å¼çš„æ•°æ®
    """
    if not scraper_data or "articles" not in scraper_data:
        return scraper_data

    converted_articles = []

    for article in scraper_data["articles"]:
        try:
            # è½¬æ¢å•ç¯‡æ–‡ç« 
            converted_article = convert_scraper_article_for_docx(article)
            if converted_article:
                converted_articles.append(converted_article)
        except Exception as e:
            unified_print(f"è½¬æ¢Scraperæ–‡ç«  {article.get('index', 'unknown')} æ—¶å‡ºé”™: {e}", "news2docx_cli", "format_conversion", "warning")
            continue

    # è¿”å›è½¬æ¢åçš„æ•°æ®ç»“æ„
    return {
        "articles": converted_articles,
        "metadata": {
            "total": scraper_data.get("total", 0),
            "success": scraper_data.get("success", 0),
            "failed": scraper_data.get("failed", 0),
            "processing_method": "scraper_only",
            "target_language": "none"
        },
        "total": scraper_data.get("total", 0),
        "success": scraper_data.get("success", 0),
        "failed": scraper_data.get("failed", 0)
    }


def convert_scraper_article_for_docx(article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    å°†Scraperå•ç¯‡æ–‡ç« è½¬æ¢ä¸ºNews2Docx_CLIæ ¼å¼

    Args:
        article: Scraperæ–‡ç« æ•°æ®

    Returns:
        Optional[Dict[str, Any]]: è½¬æ¢åçš„æ–‡ç« æ•°æ®
    """
    try:
        article_id = str(article.get("index", "unknown"))

        # è·å–æ ‡é¢˜å’Œå†…å®¹
        eng_title = article.get("title", "").strip()
        eng_content_raw = article.get("content", "")

        # å¯¹äºScraperè¾“å‡ºï¼Œæ²¡æœ‰ä¸­æ–‡å†…å®¹ï¼Œä½¿ç”¨è‹±æ–‡ä½œä¸ºé»˜è®¤
        chi_title = eng_title
        chi_content_raw = eng_content_raw

        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹
        eng_content_dict = split_content_into_paragraphs(eng_content_raw, "english")
        chi_content_dict = split_content_into_paragraphs(chi_content_raw, "chinese")

        # æ„å»ºè½¬æ¢åçš„æ–‡ç« æ•°æ®
        converted_article = {
            "id": article_id,
            "EngTitle": eng_title,
            "ChiTitle": chi_title,
            "EngContent": eng_content_dict,
            "ChiContent": chi_content_dict,
            "url": article.get("url", ""),
            "scraped_at": article.get("scraped_at", ""),
            "content_length": article.get("content_length", 0),
            "word_count": article.get("word_count", 0),
            "success": True
        }

        return converted_article

    except Exception as e:
        unified_print(f"è½¬æ¢Scraperæ–‡ç« æ—¶å‡ºé”™: {e}", "news2docx_cli", "format_conversion", "error")
        return None


# -------------------------------
# æ–‡æ¡£é…ç½®ï¼ˆNews2Docxä¸“ç”¨ï¼‰
# -------------------------------

# æ–‡æ¡£é…ç½®
DEFAULT_FIRST_LINE_INDENT = 0.3
DEFAULT_FONT_ZH_NAME = "å®‹ä½“"
DEFAULT_FONT_EN_NAME = "Cambria"
# å­—ä½“å¤§å°é…ç½®ï¼ˆäº”å·å­—ä½“ï¼‰
DEFAULT_FONT_SIZE_PT = 10.5  # äº”å·å­—ä½“ = 10.5pt
DEFAULT_TITLE_SIZE_MULTIPLIER = 1.0  # æ ‡é¢˜å’Œæ­£æ–‡ä¸€æ ·å¤§ï¼Œéƒ½æ˜¯10.5pt

# -------------------------------
# æ•°æ®ç±»å®šä¹‰ï¼ˆNews2Docxä¸“ç”¨ï¼‰
# -------------------------------

def create_output_filename(prefix: str = "", suffix: str = "", extension: str = "") -> str:
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å"""
    parts = []
    if prefix:
        parts.append(prefix)
    parts.append(now_stamp())
    if suffix:
        parts.append(suffix)

    filename = "_".join(parts)
    if extension and not extension.startswith('.'):
        extension = f".{extension}"

    return f"{filename}{extension}"


# -------------------------------
# å·¥å…·ç±»å®šä¹‰
# -------------------------------

# -------------------------------
# çˆ¬è™«ä¸»ä½“
# -------------------------------




# -------------------------------
# CLI
# -------------------------------




def run_news_pipeline(cfg: ScrapeConfig) -> ScrapeResults:
    """
    è¿è¡Œå®Œæ•´çš„æ–°é—»å¤„ç†ç®¡é“ï¼šçˆ¬å– -> AIå¤„ç† -> ä¿å­˜ç»“æœ

    Args:
        cfg: é…ç½®å¯¹è±¡

    Returns:
        ScrapeResults: çˆ¬å–ç»“æœ
    """
    # è®°å½•ä»»åŠ¡å¼€å§‹
    log_task_start("news2docx_cli", "pipeline_processing", {
        "max_urls": cfg.max_urls,
        "concurrency": cfg.concurrency,
        "target_word_range": f"{TARGET_WORD_MIN}-{TARGET_WORD_MAX}"
    })

    unified_print("å¼€å§‹æ–°é—»çˆ¬å–å’ŒAIå¤„ç†...", "news2docx_cli", "pipeline_processing")

    try:
        # è¿è¡Œçˆ¬è™«
        scraper = ScraperNewsScraper(cfg)
        scrape_results = scraper.run()

        # å¦‚æœæœ‰æˆåŠŸæŠ“å–çš„æ–‡ç« ï¼Œè¿›è¡ŒAIå¤„ç†
        if scrape_results.articles:
            unified_print(f"å¼€å§‹ä¸¤æ­¥AIå¤„ç† {len(scrape_results.articles)} ç¯‡æ–°é—»...", "news2docx_cli", "pipeline_processing")
            unified_print(f"ç¬¬ä¸€æ­¥ï¼šè¯æ•°è°ƒæ•´ ({TARGET_WORD_MIN}-{TARGET_WORD_MAX}è¯)", "news2docx_cli", "pipeline_processing")
            unified_print("ç¬¬äºŒæ­¥ï¼šç¿»è¯‘ä¸ºä¸­æ–‡", "news2docx_cli", "pipeline_processing")

            try:
                # ä½¿ç”¨æ–°çš„ä¸¤æ­¥å¤„ç†æ–¹æ³•
                processed_data = process_articles_two_steps_concurrent(scrape_results.articles, target_lang="Chinese")

                article_count = len(processed_data.get('articles', []))
                unified_print(f"ä¸¤æ­¥AIå¤„ç†å®Œæˆï¼Œå¤„ç†äº† {article_count} ç¯‡æ–°é—»", "news2docx_cli", "pipeline_processing")

                # è½¬æ¢æ ¼å¼ä»¥å…¼å®¹æ–‡æ¡£ç”Ÿæˆ
                unified_print("æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼ä»¥å…¼å®¹æ–‡æ¡£ç”Ÿæˆ...", "news2docx_cli", "pipeline_processing")
                converted_data = convert_ai_processor_output_to_docx_format(processed_data)
                unified_print(f"æ ¼å¼è½¬æ¢å®Œæˆï¼Œå…± {len(converted_data.get('articles', []))} ç¯‡æ–‡ç« ", "news2docx_cli", "pipeline_processing")

                # è®°å½•ä»»åŠ¡ç»“æŸ
                log_task_end("news2docx_cli", "pipeline_processing", True, {
                    "scraped_articles": len(scrape_results.articles),
                    "processed_articles": article_count,
                    "converted_articles": len(converted_data.get('articles', [])),
                    "target_language": "Chinese"
                })

                return converted_data

            except Exception as e:
                handle_error(e, "ä¸¤æ­¥AIå¤„ç†å¤±è´¥", "news2docx_cli", "pipeline_processing")
                # è¿”å›è½¬æ¢åçš„åŸå§‹æŠ“å–ç»“æœ
                unified_print("AIå¤„ç†å¤±è´¥ï¼Œè¿”å›è½¬æ¢åçš„åŸå§‹æŠ“å–ç»“æœ", "news2docx_cli", "pipeline_processing", "warning")

                log_task_end("news2docx_cli", "pipeline_processing", False, {
                    "error": "AIå¤„ç†å¤±è´¥",
                    "fallback": "è½¬æ¢åçš„åŸå§‹æŠ“å–ç»“æœ"
                })

                # è½¬æ¢Scraperè¾“å‡ºæ ¼å¼
                converted_scraper_data = convert_scraper_output_to_docx_format(scrape_results.to_jsonable())
                return converted_scraper_data
        else:
            unified_print("æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œè·³è¿‡AIå¤„ç†", "news2docx_cli", "pipeline_processing", "warning")

            log_task_end("news2docx_cli", "pipeline_processing", False, {
                "error": "æ— æ–‡ç« å¯å¤„ç†"
            })

            # è½¬æ¢Scraperè¾“å‡ºæ ¼å¼
            converted_scraper_data = convert_scraper_output_to_docx_format(scrape_results.to_jsonable())
            return converted_scraper_data

    except NewsProcessingError as e:
        handle_error(e, "æ–°é—»å¤„ç†ç®¡é“æ‰§è¡Œå¤±è´¥", "news2docx_cli", "pipeline_processing")
        raise
    except Exception as e:
        handle_error(e, "æ–°é—»å¤„ç†ç®¡é“æ‰§è¡Œå¤±è´¥", "news2docx_cli", "pipeline_processing")
        raise NewsProcessingError("æ–°é—»å¤„ç†ç®¡é“æ‰§è¡Œå¤±è´¥") from e


# -------------------------------
# DOCX æ–‡æ¡£ç”Ÿæˆéƒ¨åˆ†
# -------------------------------


@dataclass
class FontConfig:
    """å­—ä½“é…ç½®ç±»"""
    name: str
    size_pt: float

    def __post_init__(self) -> None:
        """éªŒè¯é…ç½®å‚æ•°"""
        if not self.name:
            raise ValueError("å­—ä½“åç§°ä¸èƒ½ä¸ºç©º")
        if self.size_pt <= 0:
            raise ValueError("å­—ä½“å¤§å°å¿…é¡»å¤§äº0")


@dataclass
class DocumentConfig:
    """æ–‡æ¡£é…ç½®ç±»"""
    first_line_indent: float
    font_zh: FontConfig
    font_en: FontConfig

    def __post_init__(self) -> None:
        """éªŒè¯é…ç½®å‚æ•°"""
        if self.first_line_indent < 0:
            raise ValueError("é¦–è¡Œç¼©è¿›ä¸èƒ½ä¸ºè´Ÿæ•°")


class Language:
    """è¯­è¨€å¸¸é‡"""
    CHINESE = "zh"
    ENGLISH = "en"
    SUPPORTED = (CHINESE, ENGLISH)


class DocumentStyle:
    """æ–‡æ¡£æ ·å¼å¸¸é‡"""
    TITLE_SIZE_MULTIPLIER = 1.0
    DEFAULT_TEXT_COLOR = RGBColor(0, 0, 0)
    FILENAME_CLEAN_PATTERN = r'[^\w\s.-]'


class DocumentWriter:
    """
    DOCXæ–‡æ¡£å†™å…¥å™¨

    æä¾›ä¸“ä¸šçš„æ–‡æ¡£ç”Ÿæˆå’Œæ ¼å¼åŒ–åŠŸèƒ½ï¼Œæ”¯æŒï¼š
    - ä¸­è‹±æ–‡åŒè¯­æ–‡æ¡£
    - è‡ªå®šä¹‰å­—ä½“å’Œæ ·å¼
    - è‡ªåŠ¨æ–‡ä»¶åæ¸…ç†
    - é”™è¯¯å¤„ç†å’ŒéªŒè¯
    """

    def __init__(self, config: DocumentConfig):
        """
        åˆå§‹åŒ–æ–‡æ¡£å†™å…¥å™¨

        Args:
            config: æ–‡æ¡£é…ç½®å¯¹è±¡

        Raises:
            ValueError: é…ç½®å‚æ•°æ— æ•ˆæ—¶æŠ›å‡º
        """
        if not isinstance(config, DocumentConfig):
            raise TypeError("configå¿…é¡»æ˜¯DocumentConfigç±»å‹")

        self.config = config
        self.document = Document()
        self._setup_document()

    def _setup_document(self) -> None:
        """è®¾ç½®æ–‡æ¡£åŸºæœ¬å±æ€§"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šæ–‡æ¡£çº§åˆ«çš„è®¾ç½®
        pass

    def _validate_language(self, language: str) -> None:
        """
        éªŒè¯è¯­è¨€å‚æ•°

        Args:
            language: è¯­è¨€ä»£ç 

        Raises:
            ValueError: è¯­è¨€ä¸æ”¯æŒæ—¶æŠ›å‡º
        """
        if language not in Language.SUPPORTED:
            supported_str = ", ".join(Language.SUPPORTED)
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€ '{language}'ï¼Œæ”¯æŒçš„è¯­è¨€: {supported_str}")

    def _get_font_config(self, language: str) -> FontConfig:
        """
        è·å–æŒ‡å®šè¯­è¨€çš„å­—ä½“é…ç½®

        Args:
            language: è¯­è¨€ä»£ç 

        Returns:
            FontConfig: å¯¹åº”çš„å­—ä½“é…ç½®
        """
        return self.config.font_zh if language == Language.CHINESE else self.config.font_en

    def _configure_font(self, run, language: str, is_bold: bool = False) -> None:
        """
        é…ç½®å­—ä½“æ ·å¼

        Args:
            run: docx runå¯¹è±¡
            language: è¯­è¨€ä»£ç 
            is_bold: æ˜¯å¦åŠ ç²—
        """
        font_config = self._get_font_config(language)

        run.font.size = Pt(font_config.size_pt)
        run.font.bold = is_bold
        run.font.color.rgb = DocumentStyle.DEFAULT_TEXT_COLOR
        run.font.name = font_config.name

        # å…³é”®éƒ¨åˆ†ï¼šåŒæ—¶è®¾ç½®ä¸­æ–‡å’Œè¥¿æ–‡å­—ä½“ï¼Œç¡®ä¿Wordæ­£ç¡®è¯†åˆ«
        r = run._element
        rPr = r.get_or_add_rPr()
        rFonts = rPr.get_or_add_rFonts()
        rFonts.set(qn("w:eastAsia"), font_config.name)  # ä¸­æ–‡
        rFonts.set(qn("w:ascii"), font_config.name)     # è‹±æ–‡
        rFonts.set(qn("w:hAnsi"), font_config.name)     # é»˜è®¤è¥¿æ–‡

    def _clean_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦

        Args:
            filename: åŸå§‹æ–‡ä»¶å

        Returns:
            str: æ¸…ç†åçš„æ–‡ä»¶å
        """
        if not filename:
            raise ValueError("æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        # ä½¿ç”¨å·¥å…·å‡½æ•°æ¸…ç†æ–‡ä»¶å
        cleaned_name = safe_filename(filename)

        # ç¡®ä¿æœ‰æ­£ç¡®çš„æ‰©å±•å
        if not cleaned_name.lower().endswith('.docx'):
            cleaned_name += '.docx'

        return cleaned_name

    def _ensure_directory(self, filepath: Union[str, Path]) -> Path:
        """
        ç¡®ä¿ç›®å½•å­˜åœ¨

        Args:
            filepath: ç›®å½•è·¯å¾„

        Returns:
            Path: ç›®å½•è·¯å¾„å¯¹è±¡

        Raises:
            OSError: åˆ›å»ºç›®å½•å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            return ensure_directory(filepath)
        except Exception as e:
            raise OSError(f"åˆ›å»ºç›®å½•å¤±è´¥: {filepath}") from e

    def add_title(self, text: str, language: str) -> None:
        """
        æ·»åŠ æ ‡é¢˜

        Args:
            text: æ ‡é¢˜æ–‡æœ¬
            language: è¯­è¨€ä»£ç  ('zh' æˆ– 'en')

        Raises:
            ValueError: å‚æ•°æ— æ•ˆæ—¶æŠ›å‡º
        """
        self._validate_language(language)

        if not text:
            raise ValueError("æ ‡é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # æ¸…ç†æ ‡é¢˜æ–‡æœ¬ï¼Œç§»é™¤å¥å·
        cleaned_text = re.sub(r"[ã€‚\.]", "", text.strip())

        if not cleaned_text:
            raise ValueError("æ¸…ç†åçš„æ ‡é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # åˆ›å»ºæ ‡é¢˜æ®µè½
        title_paragraph = self.document.add_paragraph()
        title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # è®¾ç½®æ ‡é¢˜å­—ä½“ï¼ˆæ¯”æ­£æ–‡å¤§ä¸€äº›ï¼‰
        title_run = title_paragraph.add_run(cleaned_text)
        font_config = self._get_font_config(language)
        title_size = font_config.size_pt * DocumentStyle.TITLE_SIZE_MULTIPLIER

        # ä½¿ç”¨ç»Ÿä¸€çš„å­—ä½“é…ç½®æ–¹æ³•ï¼Œç¡®ä¿ä¸­è‹±æ–‡å­—ä½“æ­£ç¡®è®¾ç½®
        self._configure_font(title_run, language, is_bold=True)
        title_run.font.size = Pt(title_size)  # å•ç‹¬è¦†ç›–å­—å·ï¼ˆæ¯”æ­£æ–‡å¤§ï¼‰

    def add_chinese_title(self, text: str) -> None:
        """
        æ·»åŠ ä¸­æ–‡æ ‡é¢˜ï¼ˆå®‹ä½“å±…ä¸­åŠ ç²—ï¼‰

        Args:
            text: ä¸­æ–‡æ ‡é¢˜æ–‡æœ¬

        Raises:
            ValueError: æ–‡æœ¬ä¸ºç©ºæ—¶æŠ›å‡º
        """
        if not text:
            raise ValueError("ä¸­æ–‡æ ‡é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # æ¸…ç†æ ‡é¢˜æ–‡æœ¬ï¼Œç§»é™¤å¥å·
        cleaned_text = re.sub(r"[ã€‚\.]", "", text.strip())

        if not cleaned_text:
            raise ValueError("æ¸…ç†åçš„ä¸­æ–‡æ ‡é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # åˆ›å»ºæ ‡é¢˜æ®µè½
        title_paragraph = self.document.add_paragraph()
        title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # æ·»åŠ ä¸­æ–‡æ ‡é¢˜æ–‡æœ¬
        title_run = title_paragraph.add_run(cleaned_text)

        # è®¡ç®—ä¸­æ–‡æ ‡é¢˜å­—ä½“å¤§å°ï¼ˆå’Œæ­£æ–‡ä¸€æ ·å¤§ï¼‰
        # äº”å·å­—ä½“10.5pt * æ ‡é¢˜ä¹˜æ•°1.0 = 10.5ptï¼Œå’Œæ­£æ–‡ä¸€æ ·å¤§å°
        chinese_title_size = DEFAULT_FONT_SIZE_PT * DEFAULT_TITLE_SIZE_MULTIPLIER

        # è®¾ç½®ä¸­æ–‡æ ‡é¢˜æ ·å¼ï¼šå®‹ä½“ã€å±…ä¸­ã€åŠ ç²—ã€å¤§å­—ä½“
        title_run.font.name = "å®‹ä½“"  # ä¸­æ–‡å®‹ä½“
        title_run.font.size = Pt(chinese_title_size)  # ä½¿ç”¨è®¡ç®—å‡ºçš„å­—ä½“å¤§å°
        title_run.font.bold = True    # åŠ ç²—

        # ç¡®ä¿Wordæ­£ç¡®è¯†åˆ«ä¸­è‹±æ–‡å­—ä½“
        r = title_run._element
        rPr = r.get_or_add_rPr()
        rFonts = rPr.get_or_add_rFonts()
        rFonts.set(qn("w:eastAsia"), "å®‹ä½“")  # ä¸­æ–‡
        rFonts.set(qn("w:ascii"), "å®‹ä½“")     # è‹±æ–‡
        rFonts.set(qn("w:hAnsi"), "å®‹ä½“")     # é»˜è®¤è¥¿æ–‡

    def add_paragraph(self, text: str, language: str) -> None:
        """
        æ·»åŠ æ®µè½å†…å®¹

        Args:
            text: æ®µè½æ–‡æœ¬ï¼ˆå•æ®µå†…å®¹ï¼‰
            language: è¯­è¨€ä»£ç  ('zh' æˆ– 'en')

        Raises:
            ValueError: æ–‡æœ¬ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦æ—¶æŠ›å‡º
        """
        self._validate_language(language)

        if not text:
            raise ValueError("æ®µè½æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
        cleaned_text = text.strip()

        if not cleaned_text:
            raise ValueError("æ¸…ç†åçš„æ®µè½æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # åˆ›å»ºæ®µè½
        paragraph = self.document.add_paragraph()

        # è®¾ç½®é¦–è¡Œç¼©è¿›
        paragraph.paragraph_format.first_line_indent = Inches(self.config.first_line_indent)

        # æ·»åŠ æ–‡æœ¬å¹¶é…ç½®å­—ä½“
        run = paragraph.add_run(cleaned_text)
        self._configure_font(run, language)

    def add_separator(self) -> None:
        """æ·»åŠ åˆ†éš”çº¿"""
        separator = self.document.add_paragraph()
        separator.add_run("â€”" * 50)

    def save(self, filename: str, filepath: Union[str, Path] = ".") -> str:
        """
        ä¿å­˜æ–‡æ¡£

        Args:
            filename: æ–‡ä»¶å
            filepath: ä¿å­˜è·¯å¾„

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶å®Œæ•´è·¯å¾„

        Raises:
            ValueError: æ–‡ä»¶åæ— æ•ˆæ—¶æŠ›å‡º
            OSError: ä¿å­˜å¤±è´¥æ—¶æŠ›å‡º
        """
        safe_filename = self._clean_filename(filename)
        directory = self._ensure_directory(filepath)

        full_path = directory / safe_filename

        try:
            self.document.save(str(full_path))
        except Exception as e:
            raise OSError(f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {full_path}") from e

        return str(full_path)

    def get_document_stats(self) -> dict:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: åŒ…å«æ®µè½æ•°ã€å­—ç¬¦æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        paragraphs = self.document.paragraphs
        total_chars = sum(len(p.text) for p in paragraphs)

        return {
            "paragraph_count": len(paragraphs),
            "total_characters": total_chars,
            "estimated_pages": max(1, total_chars // 2000)  # ç²—ç•¥ä¼°ç®—
        }


    @staticmethod
    def clean_chinese_filename(filename: str) -> str:
        """
        æ¸…ç†ä¸­æ–‡æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦å¹¶ç¡®ä¿æ–‡ä»¶åæœ‰æ•ˆ

        Args:
            filename: åŸå§‹æ–‡ä»¶åï¼ˆé€šå¸¸æ˜¯ä¸­æ–‡æ ‡é¢˜ï¼‰

        Returns:
            str: æ¸…ç†åçš„æ–‡ä»¶å
        """
        if not filename:
            return "untitled_document.docx"

        # ä½¿ç”¨å·¥å…·å‡½æ•°æ¸…ç†æ–‡ä»¶å
        cleaned_name = safe_filename(filename)

        # ç¡®ä¿æœ‰æ­£ç¡®çš„æ‰©å±•å
        if not cleaned_name.lower().endswith('.docx'):
            cleaned_name += '.docx'

        return cleaned_name

    def _process_article_content(self, article: Dict[str, Any]) -> None:
        """
        å¤„ç†å•ç¯‡æ–‡ç« å†…å®¹çš„ç§æœ‰æ–¹æ³•

        Args:
            article: å•ç¯‡æ–‡ç« æ•°æ®
        """
        start_time = time.time()
        # è·å–æ–‡ç« ID
        article_id = article.get("id", "unknown")

        input_data = {
            "article_id": article_id,
            "eng_title": article.get("EngTitle", ""),
            "chi_title": article.get("ChiTitle", ""),
            "eng_content_keys": list(article.get("EngContent", {}).keys()),
            "chi_content_keys": list(article.get("ChiContent", {}).keys())
        }

        # å¤„ç†è‹±æ–‡æ ‡é¢˜
        eng_title = article.get("EngTitle", "").strip()
        if eng_title:
            log_processing_step("news2docx_cli", "document_creation", f"æ·»åŠ æ–‡ç«  {article_id} çš„è‹±æ–‡æ ‡é¢˜", {
                "title": eng_title[:50]
            })
            self.add_title(eng_title, Language.ENGLISH)

        # å¤„ç†è‹±æ–‡å†…å®¹æ®µè½
        eng_content = article.get("EngContent", {})
        eng_paragraphs_added = 0
        if isinstance(eng_content, dict):
            # æŒ‰ p1, p2, p3... çš„é¡ºåºå¤„ç†æ®µè½
            paragraph_keys = sorted([k for k in eng_content.keys() if k.startswith("p") and k[1:].isdigit()],
                                  key=lambda x: int(x[1:]))
            for key in paragraph_keys:
                content = eng_content[key].strip()
                if content:
                    log_processing_step("news2docx_cli", "document_creation", f"æ·»åŠ æ–‡ç«  {article_id} çš„è‹±æ–‡æ®µè½ {key}")
                    self.add_paragraph(content, Language.ENGLISH)
                    eng_paragraphs_added += 1

        # å¤„ç†ä¸­æ–‡æ ‡é¢˜
        chi_title = article.get("ChiTitle", "").strip()
        if chi_title:
            log_processing_step("news2docx_cli", "document_creation", f"æ·»åŠ æ–‡ç«  {article_id} çš„ä¸­æ–‡æ ‡é¢˜", {
                "title": chi_title[:50]
            })
            # ä½¿ç”¨ä¸“é—¨çš„ä¸­æ–‡æ ‡é¢˜æ–¹æ³•ï¼ˆå®‹ä½“å±…ä¸­åŠ ç²—ï¼‰
            self.add_chinese_title(chi_title)

        # å¤„ç†ä¸­æ–‡å†…å®¹æ®µè½
        chi_content = article.get("ChiContent", {})
        chi_paragraphs_added = 0
        if isinstance(chi_content, dict):
            # æŒ‰ p1, p2, p3... çš„é¡ºåºå¤„ç†æ®µè½
            paragraph_keys = sorted([k for k in chi_content.keys() if k.startswith("p") and k[1:].isdigit()],
                                  key=lambda x: int(x[1:]))
            for key in paragraph_keys:
                content = chi_content[key].strip()
                if content:
                    log_processing_step("news2docx_cli", "document_creation", f"æ·»åŠ æ–‡ç«  {article_id} çš„ä¸­æ–‡æ®µè½ {key}")
                    self.add_paragraph(content, Language.CHINESE)
                    chi_paragraphs_added += 1

        processing_time = time.time() - start_time
        output_data = {
            "eng_title_added": bool(eng_title),
            "chi_title_added": bool(chi_title),
            "eng_paragraphs_added": eng_paragraphs_added,
            "chi_paragraphs_added": chi_paragraphs_added,
            "processing_time": processing_time
        }

        log_processing_result("news2docx_cli", "document_creation", f"æ–‡ç«  {article_id} å†…å®¹å¤„ç†",
                            input_data, output_data, "success",
                            {"processing_time": processing_time})

    def generate_single_article(self, article: Dict[str, Any]) -> None:
        """
        æ ¹æ®å•ç¯‡æ–‡ç« æ•°æ®ç”Ÿæˆ DOCX æ–‡æ¡£å†…å®¹

        Args:
            article: å•ç¯‡æ–‡ç« æ•°æ®
        """
        try:
            self._process_article_content(article)
        except Exception as e:
            article_id = article.get("id", "unknown") if isinstance(article, dict) else "unknown"
            print(f"å¤„ç†æ–‡ç«  {article_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate_from_json(self, json_data: Dict[str, Any]) -> None:
        """
        æ ¹æ® JSON æ•°æ®ç”Ÿæˆ DOCX æ–‡æ¡£

        Args:
            json_data: åŒ…å«æ–‡ç« æ•°æ®çš„ JSON å­—å…¸
        """
        start_time = time.time()
        processed_articles = 0
        failed_articles = 0

        if not json_data or "articles" not in json_data:
            log_error("news2docx_cli", "document_creation", Exception("JSON æ•°æ®æ ¼å¼æ— æ•ˆ"), "ç¼ºå°‘ 'articles' å­—æ®µ")
            return

        articles = json_data["articles"]
        if not isinstance(articles, list):
            log_error("news2docx_cli", "document_creation", Exception("'articles' å­—æ®µä¸æ˜¯åˆ—è¡¨æ ¼å¼"), "æ•°æ®æ ¼å¼é”™è¯¯")
            return

        log_processing_step("news2docx_cli", "document_creation", f"å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ")

        for i, article in enumerate(articles, 1):
            try:
                # ä½¿ç”¨ç§æœ‰æ–¹æ³•å¤„ç†æ–‡ç« å†…å®¹
                self._process_article_content(article)

                # åœ¨æ–‡ç« ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åæ–‡ç« ï¼‰
                if i < len(articles):
                    self.add_separator()

                processed_articles += 1

            except Exception as e:
                article_id = article.get("id", str(i))
                log_error("news2docx_cli", "document_creation", e, f"å¤„ç†æ–‡ç«  {article_id} æ—¶å‘ç”Ÿé”™è¯¯")
                failed_articles += 1
                continue

        processing_time = time.time() - start_time

        # è®°å½•æ‰¹é‡å¤„ç†ç»“æœ
        log_batch_processing(
            "news2docx_cli", "document_creation", "æ–‡ç« å†…å®¹å¤„ç†",
            len(articles), processed_articles, failed_articles, processing_time, "completed"
        )

        log_processing_step("news2docx_cli", "document_creation", "JSON æ•°æ®å¤„ç†å®Œæˆ", {
            "total_articles": len(articles),
            "processed_articles": processed_articles,
            "failed_articles": failed_articles,
            "processing_time": processing_time
        })

    @classmethod
    def create_from_json_data(cls, json_data: Dict[str, Any],
                             output_dir: Union[str, Path] = None) -> List[str]:
        """
        ä»å†…å­˜ä¸­çš„ JSON æ•°æ®ä¸ºæ¯ç¯‡æ–‡ç« åˆ›å»ºç‹¬ç«‹çš„ DOCX æ–‡æ¡£

        Args:
            json_data: JSON æ•°æ®å­—å…¸
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç”¨æˆ·æ¡Œé¢çš„"è‹±è¯­æ–°é—»"æ–‡ä»¶å¤¹

        Returns:
            List[str]: ç”Ÿæˆçš„ DOCX æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        start_time = time.time()

        # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•ä¸ºç”¨æˆ·æ¡Œé¢çš„"è‹±è¯­æ–°é—»"æ–‡ä»¶å¤¹
        if output_dir is None:
            import os
            desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
            output_dir = os.path.join(desktop_path, "è‹±è¯­æ–°é—»")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(output_dir)
        try:
            output_path.mkdir(parents=True, exist_ok=True)
            log_processing_step("news2docx_cli", "batch_document_creation", f"è¾“å‡ºç›®å½•: {output_path}")
        except Exception as e:
            log_error("news2docx_cli", "batch_document_creation", e, "åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥")
            return []

        # éªŒè¯ JSON æ•°æ®
        if not json_data:
            log_error("news2docx_cli", "batch_document_creation", Exception("JSON æ•°æ®ä¸ºç©º"), "æ•°æ®éªŒè¯å¤±è´¥")
            return []

        if "articles" not in json_data or not isinstance(json_data["articles"], list):
            log_error("news2docx_cli", "batch_document_creation", Exception("JSON æ•°æ®æ ¼å¼æ— æ•ˆ"), "ç¼ºå°‘æœ‰æ•ˆçš„ 'articles' å­—æ®µ")
            return []

        articles = json_data["articles"]
        generated_files = []

        log_processing_step("news2docx_cli", "batch_document_creation", f"å¼€å§‹ä¸º {len(articles)} ç¯‡æ–‡ç« ç”Ÿæˆç‹¬ç«‹çš„ DOCX æ–‡æ¡£")

        for i, article in enumerate(articles, 1):
            try:
                # è·å–ä¸­æ–‡æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
                chi_title = article.get("ChiTitle", "").strip()
                if not chi_title:
                    # å¦‚æœæ²¡æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜
                    eng_title = article.get("EngTitle", "").strip()
                    if eng_title:
                        chi_title = eng_title
                    else:
                        # å¦‚æœéƒ½æ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤åç§°
                        article_id = article.get("id", str(i))
                        chi_title = f"æ–‡ç« _{article_id}"

                # æ¸…ç†æ–‡ä»¶å
                filename = cls.clean_chinese_filename(chi_title)

                log_processing_step("news2docx_cli", "batch_document_creation", f"æ­£åœ¨å¤„ç†ç¬¬ {i} ç¯‡æ–‡ç« ", {
                    "title": chi_title,
                    "article_id": article.get("id", str(i))
                })

                # åˆ›å»ºæ–°çš„æ–‡æ¡£å†™å…¥å™¨å®ä¾‹
                config = create_default_config()
                writer = cls(config)

                # ç”Ÿæˆå•ç¯‡æ–‡ç« å†…å®¹
                writer.generate_single_article(article)

                # ä¿å­˜æ–‡æ¡£
                file_save_start = time.time()
                try:
                    output_file_path = writer.save(filename, output_path)
                    file_save_time = time.time() - file_save_start

                    # è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
                    stats = writer.get_document_stats()

                    # è®°å½•æ–‡ä»¶æ“ä½œç»“æœ
                    log_file_operation(
                        "news2docx_cli", "batch_document_creation", "ä¿å­˜DOCXæ–‡æ¡£",
                        output_file_path, stats.get("total_characters", 0), file_save_time, "success",
                        {
                            "article_id": article.get("id", str(i)),
                            "paragraph_count": stats.get("paragraph_count", 0),
                            "estimated_pages": stats.get("estimated_pages", 0)
                        }
                    )

                    log_processing_step("news2docx_cli", "batch_document_creation", f"æ–‡æ¡£å·²ç”Ÿæˆ", {
                        "file_path": output_file_path,
                        "article_id": article.get("id", str(i)),
                        "file_size": stats.get("total_characters", 0),
                        "save_time": file_save_time
                    })

                    generated_files.append(output_file_path)

                except Exception as e:
                    file_save_time = time.time() - file_save_start

                    log_file_operation(
                        "news2docx_cli", "batch_document_creation", "ä¿å­˜DOCXæ–‡æ¡£",
                        filename, 0, file_save_time, "error",
                        {"error": str(e), "article_id": article.get("id", str(i))}
                    )

                    log_error("news2docx_cli", "batch_document_creation", e, f"ä¿å­˜æ–‡æ¡£å¤±è´¥ - æ–‡ç«  {article.get('id', str(i))}")
                    continue

            except Exception as e:
                article_id = article.get("id", str(i))
                log_error("news2docx_cli", "batch_document_creation", e, f"å¤„ç†æ–‡ç«  {article_id} æ—¶å‘ç”Ÿé”™è¯¯")
                continue

        # è®°å½•å®Œæ•´çš„æ‰¹é‡å¤„ç†ç»“æœ
        log_batch_processing(
            "news2docx_cli", "batch_document_creation", "æ‰¹é‡DOCXæ–‡æ¡£ç”Ÿæˆ",
            len(articles), len(generated_files), len(articles) - len(generated_files),
            time.time() - start_time, "completed"
        )

        log_processing_step("news2docx_cli", "batch_document_creation", "æ‰¹é‡æ–‡æ¡£ç”Ÿæˆå®Œæˆ", {
            "total_files": len(generated_files),
            "output_directory": str(output_path)
        })

        return generated_files


def main() -> None:
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°ï¼Œè§£æå‚æ•°å¹¶è¿è¡Œå®Œæ•´æµç¨‹"""
    import Scraper
    ap = Scraper.build_arg_parser()
    args = ap.parse_args()

    cfg = ScrapeConfig(
        output_dir=args.output_dir,
        api_url=args.api_url,
        api_token=args.api_token,
        max_urls=max(1, args.max_urls),
        concurrency=max(1, args.concurrency),
        retry_interval_hours=max(1, args.retry_hours),
        request_timeout=max(5, args.timeout),
        strict_success=bool(args.strict_success),
        max_api_rounds=max(1, args.max_api_rounds),
        per_url_retries=max(0, args.per_url_retries),
        pick_mode=args.pick_mode,
        random_seed=args.random_seed,
    )

    run_complete_pipeline(cfg)


def create_default_config() -> DocumentConfig:
    """
    åˆ›å»ºé»˜è®¤æ–‡æ¡£é…ç½®

    Returns:
        DocumentConfig: é»˜è®¤é…ç½®å¯¹è±¡
    """
    return DocumentConfig(
        first_line_indent=0.3,
        font_zh=FontConfig(name="å®‹ä½“", size_pt=10.5),
        font_en=FontConfig(name="Cambria", size_pt=10.5)
    )


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›æ—§çš„ç±»å
ArticleProcessor = DocumentWriter


# -------------------------------
# ä¸»ç¨‹åºå…¥å£
# -------------------------------


def print_header() -> None:
    """æ‰“å°ç¨‹åºå¤´éƒ¨ä¿¡æ¯"""
    print(MAIN_SEPARATOR)
    print("æ–°é—»çˆ¬è™«ä¸æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ")
    print(MAIN_SEPARATOR)
    unified_print("æ–°é—»çˆ¬è™«ä¸æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿå¯åŠ¨", "news2docx_cli", "system_init")


def run_document_generation(json_data: Dict[str, Any]) -> List[str]:
    """è¿è¡Œæ–‡æ¡£ç”Ÿæˆæµç¨‹"""
    log_processing_step("news2docx_cli", "document_generation", "å¼€å§‹ç”ŸæˆDOCXæ–‡æ¡£")
    unified_print("å¼€å§‹ç”ŸæˆDOCXæ–‡æ¡£...", "news2docx_cli", "document_generation")

    generated_files = DocumentWriter.create_from_json_data(json_data=json_data)
    return generated_files


def display_results(generated_files: List[str]) -> None:
    """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
    unified_print("å¤„ç†ç»“æœ:", "news2docx_cli", "result_display")

    if generated_files:
        unified_print(f"å¤„ç†å®Œæˆï¼æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ª DOCX æ–‡æ¡£", "news2docx_cli", "result_display")
        unified_print("ç”Ÿæˆçš„æ–‡ä»¶:", "news2docx_cli", "result_display")
        for i, file_path in enumerate(generated_files, 1):
            unified_print(f"{i}. {file_path}", "news2docx_cli", "result_display")
    else:
        unified_print("æ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯", "news2docx_cli", "result_display", "error")


def handle_system_errors(func):
    """ç³»ç»Ÿçº§é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            sys.exit(1)
        except ImportError as e:
            print(f"\nâŒ æ¨¡å—å¯¼å…¥é”™è¯¯: {e}")
            print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…")
            sys.exit(1)
        except NewsProcessingError as e:
            handle_error(e, "æ–°é—»å¤„ç†ç³»ç»Ÿé”™è¯¯")
            sys.exit(1)
        except Exception as e:
            handle_error(e, "ç¨‹åºæ‰§è¡Œå‡ºé”™")
            sys.exit(1)
    return wrapper


@handle_system_errors
def run_complete_pipeline(cfg: Optional[ScrapeConfig] = None) -> None:
    """è¿è¡Œå®Œæ•´çš„æ–°é—»å¤„ç†æµç¨‹ï¼šçˆ¬å– -> AIå¤„ç† -> æ–‡æ¡£ç”Ÿæˆ"""
    # è®°å½•ä¸»ä»»åŠ¡å¼€å§‹
    log_task_start("news2docx_cli", "complete_pipeline", {
        "pipeline_steps": ["æ–°é—»çˆ¬å–", "AIå¤„ç†", "æ–‡æ¡£ç”Ÿæˆ"]
    })

    print_header()

    # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–æä¾›çš„é…ç½®
    cfg = cfg or ScrapeConfig()

    # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œæ–°é—»çˆ¬è™«å’ŒAIå¤„ç†ç®¡é“ï¼Œè¿”å›å¤„ç†åçš„æ•°æ®
    processed_data = run_news_pipeline(cfg)

    # ç¬¬äºŒæ­¥ï¼šç”ŸæˆDOCXæ–‡æ¡£ï¼ˆå¦‚æœæœ‰æ–‡ç« è¢«å¤„ç†ï¼‰
    if processed_data and processed_data.get("articles"):
        generated_files = run_document_generation(processed_data)
        display_results(generated_files)

        # è®°å½•ä¸»ä»»åŠ¡ç»“æŸ
        log_task_end("news2docx_cli", "complete_pipeline", True, {
            "total_steps": 3,
            "generated_files_count": len(generated_files),
            "processed_articles": len(processed_data.get("articles", []))
        })
    else:
        unified_print("æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°æ–‡ç« ï¼Œè·³è¿‡æ–‡æ¡£ç”Ÿæˆ", "news2docx_cli", "complete_pipeline", "warning")

        # è®°å½•ä¸»ä»»åŠ¡ç»“æŸ
        log_task_end("news2docx_cli", "complete_pipeline", False, {
            "error": "æ— æ–‡ç« æ•°æ®",
            "completed_steps": 1
        })


# -------------------------------
# Scraperé›†æˆåŠŸèƒ½
# -------------------------------



# -------------------------------
# Scraperé›†æˆåŠŸèƒ½
# -------------------------------

def run_with_scraper(cfg: Optional[ScrapeConfig] = None) -> None:
    """
    ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œæ–°é—»çˆ¬å–ï¼Œç„¶ååœ¨News2Docx_CLI.pyä¸­è¿›è¡ŒAIå¤„ç†

    Args:
        scraper_cfg: Scraperæ¨¡å—çš„é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    print("=" * 60)
    print("ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œæ–°é—»çˆ¬å–")
    print("=" * 60)

    try:
        # åŠ¨æ€å¯¼å…¥Scraperæ¨¡å—
        import Scraper

        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–æä¾›çš„é…ç½®
        cfg = cfg or Scraper.ScrapeConfig()

        print("ğŸ” å¼€å§‹ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œæ–°é—»çˆ¬å–...")
        print("-" * 40)

        # åˆ›å»ºScraperå®ä¾‹ï¼ˆçº¯çˆ¬è™«æ¨¡å¼ï¼‰
        scraper = ScraperNewsScraper(cfg)

        # è¿è¡Œçº¯çˆ¬è™«
        scrape_results = scraper.run()

        print(f"\nâœ… çˆ¬å–å®Œæˆï¼æˆåŠŸæŠ“å– {len(scrape_results.articles)} ç¯‡æ–‡ç« ")

        # ä¿å­˜çˆ¬å–æ•°æ®ä¸ºJSONæ–‡ä»¶
        if scrape_results.success > 0:
            timestamp = now_stamp()
            save_scraped_data_to_json(scrape_results, timestamp)

        # å°†çˆ¬å–ç»“æœä¼ é€’ç»™News2Docx_CLI.pyçš„AIå¤„ç†æµç¨‹
        if scrape_results.articles:
            print(f"\nå¼€å§‹AIå¤„ç† {len(scrape_results.articles)} ç¯‡æ–°é—»...")

            try:
                # ä½¿ç”¨æ–°çš„ä¸¤æ­¥AIå¤„ç†æµç¨‹
                processed_data = process_articles_two_steps_concurrent(scrape_results.articles, target_lang="Chinese")

                print(f"[æˆåŠŸ] ä¸¤æ­¥AIå¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(processed_data.get('articles', []))} ç¯‡æ–°é—»")

                # è½¬æ¢æ ¼å¼ä»¥å…¼å®¹æ–‡æ¡£ç”Ÿæˆ
                print("[ä¿¡æ¯] æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼ä»¥å…¼å®¹æ–‡æ¡£ç”Ÿæˆ...")
                converted_data = convert_ai_processor_output_to_docx_format(processed_data)
                print(f"[æˆåŠŸ] æ ¼å¼è½¬æ¢å®Œæˆï¼Œå…± {len(converted_data.get('articles', []))} ç¯‡æ–‡ç« ")

                # ç”ŸæˆDOCXæ–‡æ¡£
                if converted_data and converted_data.get("articles"):
                    generated_files = run_document_generation(converted_data)
                    display_results(generated_files)
                else:
                    print("\n[è­¦å‘Š] æ²¡æœ‰æˆåŠŸå¤„ç†åˆ°æ–‡ç« ï¼Œè·³è¿‡æ–‡æ¡£ç”Ÿæˆ")
                    print(MAIN_SEPARATOR)

            except Exception as e:
                handle_error(e, "AIå¤„ç†å¤±è´¥")
                # è¿”å›è½¬æ¢åçš„åŸå§‹æŠ“å–ç»“æœ
                print("[å¤‡é€‰] AIå¤„ç†å¤±è´¥ï¼Œè¿”å›è½¬æ¢åçš„åŸå§‹æŠ“å–ç»“æœ")
                converted_scraper_data = convert_scraper_output_to_docx_format(scrape_results.to_jsonable())
                if converted_scraper_data and converted_scraper_data.get("articles"):
                    generated_files = run_document_generation(converted_scraper_data)
                    display_results(generated_files)
        else:
            print("\n[è­¦å‘Š] æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œè·³è¿‡AIå¤„ç†")
            print(MAIN_SEPARATOR)

    except NewsProcessingError as e:
        handle_error(e, "Scraperæ¨¡å—æ‰§è¡Œå¤±è´¥")
        raise
    except Exception as e:
        handle_error(e, "Scraperæ¨¡å—æ‰§è¡Œå¤±è´¥")
        raise NewsProcessingError("Scraperæ¨¡å—æ‰§è¡Œå¤±è´¥") from e


def build_scraper_arg_parser() -> argparse.ArgumentParser:
    """æ„å»ºScraperä¸“ç”¨å‚æ•°è§£æå™¨"""
    p = argparse.ArgumentParser(description="ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œæ–°é—»çˆ¬å–ï¼ˆçº¯çˆ¬è™«æ¨¡å¼ï¼‰ï¼Œç„¶åè¿›è¡ŒAIå¤„ç†")
    p.add_argument("--output-dir", default=os.getenv("CRAWLER_OUTPUT_DIR", ""))
    p.add_argument("--api-url", default=os.getenv("CRAWLER_API_URL", None))
    p.add_argument("--api-token", default=os.getenv("CRAWLER_API_TOKEN", None))
    p.add_argument("--max-urls", type=int, default=int(os.getenv("CRAWLER_MAX_URLS", "10")))
    p.add_argument("--concurrency", type=int, default=int(os.getenv("CRAWLER_CONCURRENCY", str(DEFAULT_CONCURRENCY))))
    p.add_argument("--retry-hours", type=int, default=int(os.getenv("CRAWLER_RETRY_HOURS", "24")))
    p.add_argument("--timeout", type=int, default=int(os.getenv("CRAWLER_TIMEOUT", "30")))
    p.add_argument("--strict-success", action="store_true", default=True)
    p.add_argument("--max-api-rounds", type=int, default=int(os.getenv("CRAWLER_MAX_API_ROUNDS", "5")))
    p.add_argument("--per-url-retries", type=int, default=int(os.getenv("CRAWLER_PER_URL_RETRIES", "2")))
    p.add_argument("--pick-mode", choices=["fifo", "random"], default=os.getenv("CRAWLER_PICK_MODE", "random"),
                   help="URL å–æ ·æ¨¡å¼ï¼šfifo æˆ– randomï¼ˆé»˜è®¤ randomï¼‰")
    p.add_argument("--random-seed", type=int, default=(int(os.getenv("CRAWLER_RANDOM_SEED")) if os.getenv("CRAWLER_RANDOM_SEED") else None),
                   help="éšæœºç§å­ï¼Œç”¨äº random æ¨¡å¼ä¸‹ç»“æœå¯å¤ç°")
    p.add_argument("--use-scraper", action="store_true", help="ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œçˆ¬å–ï¼ˆAIå¤„ç†ä»åœ¨News2Docx_CLI.pyä¸­ï¼‰")

    return p


def main_with_scraper() -> None:
    """ä½¿ç”¨Scraperæ¨¡å—çš„å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    ap = build_scraper_arg_parser()
    args = ap.parse_args()

    # åŠ¨æ€å¯¼å…¥Scraperæ¨¡å—
    import Scraper

    cfg = Scraper.ScrapeConfig(
        output_dir=args.output_dir,
        api_url=args.api_url,
        api_token=args.api_token,
        max_urls=max(1, args.max_urls),
        concurrency=max(1, args.concurrency),
        retry_interval_hours=max(1, args.retry_hours),
        request_timeout=max(5, args.timeout),
        strict_success=bool(args.strict_success),
        max_api_rounds=max(1, args.max_api_rounds),
        per_url_retries=max(0, args.per_url_retries),
        pick_mode=args.pick_mode,
        random_seed=args.random_seed,
    )

    run_with_scraper(cfg)


def test_font_sizes():
    """æµ‹è¯•å­—ä½“å¤§å°è®¾ç½®"""
    print("æµ‹è¯•å­—ä½“å¤§å°è®¾ç½®...")

    try:
        # è®¡ç®—å„ç§å­—ä½“å¤§å°
        base_size = DEFAULT_FONT_SIZE_PT  # äº”å·å­—ä½“ = 10.5pt
        title_multiplier = DEFAULT_TITLE_SIZE_MULTIPLIER  # 1.0
        chinese_title_size = base_size * title_multiplier  # 10.5 * 1.0 = 10.5pt

        print(f"âœ… æ­£æ–‡å­—ä½“å¤§å°ï¼ˆäº”å·ï¼‰: {base_size}pt")
        print(f"âœ… æ ‡é¢˜æ”¾å¤§ä¹˜æ•°: {title_multiplier}")
        print(f"âœ… è‹±æ–‡æ ‡é¢˜å¤§å°: {base_size * title_multiplier:.1f}pt (å’Œæ­£æ–‡ä¸€æ ·å¤§)")
        print(f"âœ… ä¸­æ–‡æ ‡é¢˜å¤§å°: {chinese_title_size:.1f}pt (å’Œæ­£æ–‡ä¸€æ ·å¤§)")

        # åˆ›å»ºé…ç½®å¹¶æµ‹è¯•
        config = create_default_config()
        writer = DocumentWriter(config)

        # æµ‹è¯•è‹±æ–‡æ ‡é¢˜
        writer.add_title("English Title Test", "en")

        # æµ‹è¯•ä¸­æ–‡æ ‡é¢˜
        writer.add_chinese_title("ä¸­æ–‡æ ‡é¢˜æµ‹è¯•")

        print("âœ… å­—ä½“å¤§å°è®¾ç½®æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ å­—ä½“å¤§å°æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_format_conversion():
    """æµ‹è¯•æ ¼å¼è½¬æ¢åŠŸèƒ½"""
    print("æµ‹è¯•æ ¼å¼è½¬æ¢åŠŸèƒ½...")

    # æµ‹è¯•ai_processoræ ¼å¼è½¬æ¢
    ai_processor_sample = {
        "metadata": {
            "total_articles": 1,
            "successful_articles": 1,
            "processing_method": "two_step_processing"
        },
        "articles": [
            {
                "id": "1",
                "original_title": "Test English Title",
                "translated_title": "æµ‹è¯•è‹±æ–‡æ ‡é¢˜",
                "original_content": "This is the first paragraph.\nThis is the second paragraph.",
                "adjusted_content": "This is the expanded first paragraph with more content.\nThis is the expanded second paragraph with additional details.",
                "adjusted_word_count": 450,
                "translated_content": "è¿™æ˜¯ç¬¬ä¸€ä¸ªæ®µè½ï¼Œç»è¿‡æ‰©å±•çš„å†…å®¹ã€‚\nè¿™æ˜¯ç¬¬äºŒä¸ªæ®µè½ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚",
                "target_language": "Chinese",
                "processing_timestamp": "20241201_120000",
                "url": "https://example.com/test",
                "success": True
            }
        ]
    }

    # æµ‹è¯•Scraperæ ¼å¼è½¬æ¢
    scraper_sample = {
        "total": 1,
        "success": 1,
        "failed": 0,
        "articles": [
            {
                "index": 1,
                "url": "https://example.com/test",
                "title": "Test Title",
                "content": "This is test content.\nSecond paragraph here.",
                "content_length": 100,
                "word_count": 20,
                "scraped_at": "20241201_120000"
            }
        ],
        "successful_urls": ["https://example.com/test"],
        "failed_urls": []
    }

    try:
        # æµ‹è¯•ai_processorè½¬æ¢
        converted_ai = convert_ai_processor_output_to_docx_format(ai_processor_sample)
        print(f"âœ… AIå¤„ç†å™¨æ ¼å¼è½¬æ¢æˆåŠŸ: {len(converted_ai.get('articles', []))} ç¯‡æ–‡ç« ")

        # æµ‹è¯•Scraperè½¬æ¢
        converted_scraper = convert_scraper_output_to_docx_format(scraper_sample)
        print(f"âœ… Scraperæ ¼å¼è½¬æ¢æˆåŠŸ: {len(converted_scraper.get('articles', []))} ç¯‡æ–‡ç« ")

        # éªŒè¯è½¬æ¢ç»“æœæ ¼å¼
        if converted_ai.get('articles') and len(converted_ai['articles']) > 0:
            article = converted_ai['articles'][0]
            required_keys = ['id', 'EngTitle', 'ChiTitle', 'EngContent', 'ChiContent']
            missing_keys = [key for key in required_keys if key not in article]
            if missing_keys:
                print(f"âŒ AIè½¬æ¢ç»“æœç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_keys}")
            else:
                print("âœ… AIè½¬æ¢ç»“æœæ ¼å¼æ­£ç¡®")

        print("æ ¼å¼è½¬æ¢æµ‹è¯•å®Œæˆ!")
        return True

    except Exception as e:
        print(f"âŒ æ ¼å¼è½¬æ¢æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œçˆ¬å–
        if "--use-scraper" in sys.argv:
            # ä½¿ç”¨Scraperæ¨¡å—è¿›è¡Œçˆ¬å–ï¼ŒAIå¤„ç†ä»åœ¨News2Docx_CLI.pyä¸­
            main_with_scraper()
        elif "--test-conversion" in sys.argv:
            # æµ‹è¯•æ ¼å¼è½¬æ¢åŠŸèƒ½
            test_format_conversion()
        elif "--test-fonts" in sys.argv:
            # æµ‹è¯•å­—ä½“å¤§å°è®¾ç½®
            test_font_sizes()
        else:
            # ä½¿ç”¨å†…ç½®çˆ¬å–åŠŸèƒ½
            main()
    else:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œç›´æ¥è¿è¡Œå®Œæ•´æµç¨‹
        run_complete_pipeline()